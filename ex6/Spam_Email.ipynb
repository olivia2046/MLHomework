{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Machine Learning Online Class\n",
    "#  Exercise 6 | Spam Classification with SVMs\n",
    "#\n",
    "#  Instructions\n",
    "#  ------------\n",
    "# \n",
    "#  This file contains code that helps you get started on the\n",
    "#  exercise. You will need to complete the following functions:\n",
    "#\n",
    "#     gaussianKernel.m\n",
    "#     dataset3Params.m\n",
    "#     processEmail.m\n",
    "#     emailFeatures.m\n",
    "#\n",
    "#  For this exercise, you will not need to change any code in this file,\n",
    "#  or any other files other than those mentioned above.\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Email处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    #READFILE reads a file and returns its entire contents \n",
    "    #   file_contents = READFILE(filename) reads a file and returns its entire\n",
    "    #   contents in file_contents\n",
    "    #\n",
    "\n",
    "    # Load File\n",
    "    '''fobj = open(filename)\n",
    "    try:\n",
    "        file_contents = fobj.read()\n",
    "    except Exception as e:\n",
    "        print('Unable to open %s\\n'%filename);'''\n",
    "    \n",
    "    with open(filename) as fobj:\n",
    "        file_contents = fobj.read()\n",
    "    \n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVocabList():\n",
    "    #GETVOCABLIST reads the fixed vocabulary list in vocab.txt and returns a\n",
    "    #cell array of the words\n",
    "    #   vocabList = GETVOCABLIST() reads the fixed vocabulary list in vocab.txt \n",
    "    #   and returns a cell array of the words in vocabList.\n",
    "\n",
    "    # Read the fixed vocabulary list\n",
    "    #fobj = fopen('vocab.txt');\n",
    "\n",
    "    # Store all dictionary words in cell array vocab{}\n",
    "    #n = 1899;  # Total number of words in the dictionary\n",
    "\n",
    "    # For ease of implementation, we use a struct to map the strings => integers\n",
    "    # In practice, you'll want to use some form of hashmap\n",
    "    #vocabList = cell(n, 1)\n",
    "    vocabList = []\n",
    "    \n",
    "    with open('vocab.txt') as fobj:\n",
    "        for line in fobj.readlines():\n",
    "            vocabList.append(line.split('\\t')[1].strip('\\n'))\n",
    "\n",
    "    return vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "def processEmail(email_contents):\n",
    "    #PROCESSEMAIL preprocesses a the body of an email and\n",
    "    #returns a list of word_indices \n",
    "    #   word_indices = PROCESSEMAIL(email_contents) preprocesses \n",
    "    #   the body of an email and returns a list of indices of the \n",
    "    #   words contained in the email. \n",
    "    #\n",
    "\n",
    "    # Load Vocabulary\n",
    "    vocabList = getVocabList()\n",
    "\n",
    "    # Init return value\n",
    "    word_indices = []\n",
    "\n",
    "    # ========================== Preprocess Email ===========================\n",
    "\n",
    "    # Find the Headers ( \\n\\n and remove )\n",
    "    # Uncomment the following lines if you are working with raw emails with the\n",
    "    # full headers\n",
    "\n",
    "    # hdrstart = strfind(email_contents, ([char(10) char(10)]));\n",
    "    # email_contents = email_contents(hdrstart(1):end);\n",
    "\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "\n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # and does not have any < or > in the tag it with a space\n",
    "    email_contents = re.sub('<[^<>]+>',' ',email_contents)\n",
    "\n",
    "    # Handle Numbers\n",
    "    # Look for one or more characters between 0-9\n",
    "    email_contents = re.sub('[0-9]+','number',email_contents)\n",
    "\n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https://\n",
    "    email_contents = re.sub('(http|https)://[^\\s]*','httpaddr',email_contents)\n",
    "\n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+','emailaddr',email_contents)\n",
    "\n",
    "    # Handle $ sign\n",
    "    email_contents = re.sub('[$]+','dollar',email_contents)\n",
    "\n",
    "\n",
    "    # ========================== Tokenize Email ===========================\n",
    "\n",
    "    # Output the email to screen as well\n",
    "    print('\\n==== Processed Email ====\\n\\n');\n",
    "\n",
    "    # Process file\n",
    "    l = 0;\n",
    "\n",
    "    while email_contents !='':\n",
    "\n",
    "        # Tokenize and also get rid of any punctuation\n",
    "        split_list = re.split(r'[ @$/#.-:&*+=\\[\\]?!(){},\\'\">_<;% \\n\\r]',email_contents, 1) \n",
    "        #split_list = re.split(r'[ @$/#-:&*+=[]!(){},>_<;%]',email_contents, 1) \n",
    "        #split_list = re.split('[>@]',email_contents, 1) \n",
    "           #re.split([' @$/#.-:&*+=[]?!(){},''\">_<;%' char(10) char(13)],email_contents, 1); \n",
    "            # char(10)换行键 char(13)回车键\n",
    "        #print (split_list)\n",
    "        str, email_contents = split_list[0],split_list[1]\n",
    "\n",
    "        # Remove any non alphanumeric characters\n",
    "        str = re.sub('[^a-zA-Z0-9]', '', str);\n",
    "\n",
    "        # Stem the word \n",
    "        # (the porterStemmer sometimes has issues, so we use a try catch block)\n",
    "        '''try str = porterStemmer(strtrim(str)); \n",
    "        catch str = ''; continue;\n",
    "        end;'''\n",
    "        \n",
    "        try:\n",
    "            str = PorterStemmer().stem(str.strip())\n",
    "        except Exception as e:\n",
    "            print('Exception')\n",
    "        \n",
    "\n",
    "        # Skip the word if it is too short\n",
    "        if len(str) <= 1:\n",
    "           continue\n",
    "        \n",
    "\n",
    "        # Look up the word in the dictionary and add to word_indices if\n",
    "        # found\n",
    "        # ====================== YOUR CODE HERE ======================\n",
    "        # Instructions: Fill in this function to add the index of str to\n",
    "        #               word_indices if it is in the vocabulary. At this point\n",
    "        #               of the code, you have a stemmed word from the email in\n",
    "        #               the variable str. You should look up str in the\n",
    "        #               vocabulary list (vocabList). If a match exists, you\n",
    "        #               should add the index of the word to the word_indices\n",
    "        #               vector. Concretely, if str = 'action', then you should\n",
    "        #               look up the vocabulary list to find where in vocabList\n",
    "        #               'action' appears. For example, if vocabList{18} =\n",
    "        #               'action', then, you should add 18 to the word_indices \n",
    "        #               vector (e.g., word_indices = [word_indices ; 18]; ).\n",
    "        # \n",
    "        # Note: vocabList{idx} returns a the word with index idx in the\n",
    "        #       vocabulary list.\n",
    "        # \n",
    "        # Note: You can use strcmp(str1, str2) to compare two strings (str1 and\n",
    "        #       str2). It will return 1 only if the two strings are equivalent.\n",
    "        #\n",
    "        try:\n",
    "            indice = vocabList.index(str)\n",
    "            word_indices.append(indice)\n",
    "        except Exception as e:\n",
    "            print(\"%s not in vocabList\"%str)\n",
    "\n",
    "\n",
    "        # =============================================================\n",
    "\n",
    "        # Print to screen, ensuring that the output lines are not too long\n",
    "        if (l + len(str) + 1) > 78:\n",
    "            print('\\n');\n",
    "            l = 0;\n",
    "\n",
    "        print('%s '%str);\n",
    "        l = l + len(str) + 1;\n",
    "\n",
    "    # Print footer\n",
    "    print('\\n\\n=========================\\n');\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing sample email (emailSample1.txt)\n",
      "\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon \n",
      "know \n",
      "how \n",
      "much \n",
      "it \n",
      "cost \n",
      "to \n",
      "host \n",
      "web \n",
      "portal not in vocabList\n",
      "portal \n",
      "well \n",
      "it \n",
      "depend \n",
      "on \n",
      "how \n",
      "mani \n",
      "visitor not in vocabList\n",
      "\n",
      "\n",
      "visitor \n",
      "you \n",
      "re \n",
      "expect \n",
      "thi \n",
      "can \n",
      "be \n",
      "anywher \n",
      "from \n",
      "less \n",
      "than \n",
      "number \n",
      "buck not in vocabList\n",
      "buck \n",
      "month \n",
      "to \n",
      "\n",
      "\n",
      "coupl \n",
      "of \n",
      "dollarnumb \n",
      "you \n",
      "should \n",
      "checkout not in vocabList\n",
      "checkout \n",
      "httpaddr \n",
      "or \n",
      "perhap \n",
      "amazon not in vocabList\n",
      "amazon \n",
      "ecnumb not in vocabList\n",
      "ecnumb \n",
      "if \n",
      "\n",
      "\n",
      "your \n",
      "run \n",
      "someth \n",
      "big \n",
      "to \n",
      "unsubscrib \n",
      "yourself \n",
      "from \n",
      "thi \n",
      "mail \n",
      "list \n",
      "send \n",
      "an \n",
      "email \n",
      "\n",
      "\n",
      "to \n",
      "emailaddr \n",
      "\n",
      "\n",
      "=========================\n",
      "\n",
      "Word Indices: \n",
      "\n",
      "[85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 882, 430, 1170, 793, 1001, 1892, 1363, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 1698, 374, 1161, 478, 1892, 1509, 798, 1181, 1236, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 687, 1675, 991, 960, 1476, 70, 529, 1698, 530]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== Part 1: Email Preprocessing ====================\n",
    "#  To use an SVM to classify emails into Spam v.s. Non-Spam, you first need\n",
    "#  to convert each email into a vector of features. In this part, you will\n",
    "#  implement the preprocessing steps for each email. You should\n",
    "#  complete the code in processEmail.m to produce a word indices vector\n",
    "#  for a given email.\n",
    "\n",
    "print('\\nPreprocessing sample email (emailSample1.txt)\\n');\n",
    "\n",
    "# Extract Features\n",
    "file_contents = readFile('emailSample1.txt');\n",
    "word_indices  = processEmail(file_contents);\n",
    "\n",
    "# Print Stats\n",
    "print('Word Indices: \\n');\n",
    "print(word_indices)\n",
    "print('\\n\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "    #EMAILFEATURES takes in a word_indices vector and produces a feature vector\n",
    "    #from the word indices\n",
    "    #   x = EMAILFEATURES(word_indices) takes in a word_indices vector and \n",
    "    #   produces a feature vector from the word indices. \n",
    "\n",
    "    # Total number of words in the dictionary\n",
    "    #n = 1899; # hard code?\n",
    "    # Load Vocabulary\n",
    "    n = len(getVocabList())\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = np.zeros((n, 1))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Fill in this function to return a feature vector for the\n",
    "    #               given email (word_indices). To help make it easier to \n",
    "    #               process the emails, we have have already pre-processed each\n",
    "    #               email and converted each word in the email into an index in\n",
    "    #               a fixed dictionary (of 1899 words). The variable\n",
    "    #               word_indices contains the list of indices of the words\n",
    "    #               which occur in one email.\n",
    "    # \n",
    "    #               Concretely, if an email has the text:\n",
    "    #\n",
    "    #                  The quick brown fox jumped over the lazy dog.\n",
    "    #\n",
    "    #               Then, the word_indices vector for this text might look \n",
    "    #               like:\n",
    "    #               \n",
    "    #                   60  100   33   44   10     53  60  58   5\n",
    "    #\n",
    "    #               where, we have mapped each word onto a number, for example:\n",
    "    #\n",
    "    #                   the   -- 60\n",
    "    #                   quick -- 100\n",
    "    #                   ...\n",
    "    #\n",
    "    #              (note: the above numbers are just an example and are not the\n",
    "    #               actual mappings).\n",
    "    #\n",
    "    #              Your task is take one such word_indices vector and construct\n",
    "    #              a binary feature vector that indicates whether a particular\n",
    "    #              word occurs in the email. That is, x(i) = 1 when word i\n",
    "    #              is present in the email. Concretely, if the word 'the' (say,\n",
    "    #              index 60) appears in the email, then x(60) = 1. The feature\n",
    "    #              vector should look like:\n",
    "    #\n",
    "    #              x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..];\n",
    "    #\n",
    "    #\n",
    "    x = np.array([1 if i in word_indices else 0 for i in range(n)]).reshape(1,-1)\n",
    "\n",
    "    # =========================================================================\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from sample email (emailSample1.txt)\n",
      "\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon \n",
      "know \n",
      "how \n",
      "much \n",
      "it \n",
      "cost \n",
      "to \n",
      "host \n",
      "web \n",
      "portal not in vocabList\n",
      "portal \n",
      "well \n",
      "it \n",
      "depend \n",
      "on \n",
      "how \n",
      "mani \n",
      "visitor not in vocabList\n",
      "\n",
      "\n",
      "visitor \n",
      "you \n",
      "re \n",
      "expect \n",
      "thi \n",
      "can \n",
      "be \n",
      "anywher \n",
      "from \n",
      "less \n",
      "than \n",
      "number \n",
      "buck not in vocabList\n",
      "buck \n",
      "month \n",
      "to \n",
      "\n",
      "\n",
      "coupl \n",
      "of \n",
      "dollarnumb \n",
      "you \n",
      "should \n",
      "checkout not in vocabList\n",
      "checkout \n",
      "httpaddr \n",
      "or \n",
      "perhap \n",
      "amazon not in vocabList\n",
      "amazon \n",
      "ecnumb not in vocabList\n",
      "ecnumb \n",
      "if \n",
      "\n",
      "\n",
      "your \n",
      "run \n",
      "someth \n",
      "big \n",
      "to \n",
      "unsubscrib \n",
      "yourself \n",
      "from \n",
      "thi \n",
      "mail \n",
      "list \n",
      "send \n",
      "an \n",
      "email \n",
      "\n",
      "\n",
      "to \n",
      "emailaddr \n",
      "\n",
      "\n",
      "=========================\n",
      "\n",
      "Length of feature vector: 1\n",
      "\n",
      "Number of non-zero entries: 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== Part 2: Feature Extraction ====================\n",
    "#  Now, you will convert each email into a vector of features in R^n. \n",
    "#  You should complete the code in emailFeatures.m to produce a feature\n",
    "#  vector for a given email.\n",
    "\n",
    "print('\\nExtracting features from sample email (emailSample1.txt)\\n');\n",
    "\n",
    "# Extract Features\n",
    "file_contents = readFile('emailSample1.txt');\n",
    "word_indices  = processEmail(file_contents);\n",
    "features      = emailFeatures(word_indices);\n",
    "\n",
    "# Print Stats\n",
    "print('Length of feature vector: %d\\n'%len(features))\n",
    "print('Number of non-zero entries: %d\\n'%np.sum(features > 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为有重复词，所以非0元素个数为45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 用线性SVM训练垃圾邮件分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =========== Part 3: Train Linear SVM for Spam Classification ========\n",
    "#  In this section, you will train a linear classifier to determine if an\n",
    "#  email is Spam or Not-Spam.\n",
    "\n",
    "# Load the Spam Email dataset\n",
    "# You will have X, y in your environment\n",
    "data = scio.loadmat('spamTrain.mat');\n",
    "X = data['X']\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用LinearSVC训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear SVM (Spam Classification)\n",
      "\n",
      "(this may take 1 to 2 minutes) ...\n",
      "\n",
      "Training Accuracy: 99.975000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "print('\\nTraining Linear SVM (Spam Classification)\\n')\n",
    "print('(this may take 1 to 2 minutes) ...\\n')\n",
    "\n",
    "C = 0.1;\n",
    "#model = svmTrain(X, y, C, @linearKernel);\n",
    "model = LinearSVC(C=C)\n",
    "model.fit(X,y.ravel())\n",
    "\n",
    "p = model.predict(X)\n",
    "\n",
    "print('Training Accuracy: %f\\n'%(np.mean(np.double(p.reshape(-1,1) == y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行非常快（1秒 vs SVC的十几秒）\n",
    "运行多次结果也相同（未加random_state参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 测试垃圾邮件分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the trained Linear SVM on a test set ...\n",
      "\n",
      "Test Accuracy: 99.200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================== Part 4: Test Spam Classification ================\n",
    "#  After training the classifier, we can evaluate it on a test set. We have\n",
    "#  included a test set in spamTest.mat\n",
    "\n",
    "# Load the test dataset\n",
    "# You will have Xtest, ytest in your environment\n",
    "spamtest = scio.loadmat('spamTest.mat');\n",
    "\n",
    "Xtest = spamtest['Xtest']\n",
    "ytest = spamtest['ytest']\n",
    "\n",
    "print('\\nEvaluating the trained Linear SVM on a test set ...\\n')\n",
    "\n",
    "p = model.predict(Xtest)\n",
    "\n",
    "print('Test Accuracy: %f\\n'%(np.mean(np.double(p.reshape(-1,1) == ytest)) * 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 对垃圾邮件预测度最高的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictors of spam: \n",
      "\n",
      "our\n",
      "0.421665082918\n",
      "remov\n",
      "0.387173146899\n",
      "click\n",
      "0.387059738578\n",
      "basenumb\n",
      "0.346617244183\n",
      "guarante\n",
      "0.341685556413\n",
      "visit\n",
      "0.303027776237\n",
      "bodi\n",
      "0.263523552183\n",
      "will\n",
      "0.244393910157\n",
      "numberb\n",
      "0.238794974038\n",
      "price\n",
      "0.234199073925\n",
      "dollar\n",
      "0.232314845744\n",
      "nbsp\n",
      "0.227080751854\n",
      "below\n",
      "0.22319898392\n",
      "lo\n",
      "0.219993756217\n",
      "most\n",
      "0.214548559064\n"
     ]
    }
   ],
   "source": [
    "# ================= Part 5: Top Predictors of Spam ====================\n",
    "#  Since the model we are training is a linear SVM, we can inspect the\n",
    "#  weights learned by the model to understand better how it is determining\n",
    "#  whether an email is spam or not. The following code finds the words with\n",
    "#  the highest weights in the classifier. Informally, the classifier\n",
    "#  'thinks' that these words are the most likely indicators of spam.\n",
    "#\n",
    "\n",
    "# Sort the weights and obtin the vocabulary list\n",
    "#weight, idx = sort(model.coef_, 'descend');\n",
    "idx = np.argsort(-model.coef_,) # descend sorting\n",
    "#idx由model的权重参数排序位置组成\n",
    "#print(model.coef_)\n",
    "vocabList = getVocabList();\n",
    "\n",
    "print('\\nTop predictors of spam: \\n');\n",
    "for i in range(15): # 列出权重值最高的15个词\n",
    "    #print(' %-15s (%f) \\n'%(vocabList[idx[i]], weight[i]))\n",
    "    print(vocabList[idx[0,i]])\n",
    "    print(model.coef_[0,idx[0,i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和作业MATLAB程序的结果有一定差异，使用线性核函数的SVC分类器试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear SVM (Spam Classification)\n",
      " using SVC of linear kernel\n",
      "(this may take 1 to 2 minutes) ...\n",
      "\n",
      "Training Accuracy: 99.825000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "print('\\nTraining Linear SVM (Spam Classification)\\n using SVC of linear kernel')\n",
    "print('(this may take 1 to 2 minutes) ...\\n')\n",
    "\n",
    "C = 0.1;\n",
    "#model = svmTrain(X, y, C, @linearKernel);\n",
    "model = SVC(C=C,kernel = 'linear')\n",
    "model.fit(X,y.ravel())\n",
    "\n",
    "p = model.predict(X)\n",
    "\n",
    "print('Training Accuracy: %f\\n'%(np.mean(np.double(p.reshape(-1,1) == y)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the trained Linear SVM on a test set ...\n",
      "\n",
      "Test Accuracy: 98.900000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "# You will have Xtest, ytest in your environment\n",
    "spamtest = scio.loadmat('spamTest.mat');\n",
    "\n",
    "Xtest = spamtest['Xtest']\n",
    "ytest = spamtest['ytest']\n",
    "\n",
    "print('\\nEvaluating the trained Linear SVM on a test set ...\\n')\n",
    "\n",
    "p = model.predict(Xtest)\n",
    "\n",
    "print('Test Accuracy: %f\\n'%(np.mean(np.double(p.reshape(-1,1) == ytest)) * 100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictors of spam: \n",
      "\n",
      "our\n",
      "0.500613736175\n",
      "click\n",
      "0.465916390689\n",
      "remov\n",
      "0.422869117061\n",
      "guarante\n",
      "0.383621601794\n",
      "visit\n",
      "0.367710398246\n",
      "basenumb\n",
      "0.345064097946\n",
      "dollar\n",
      "0.323632035796\n",
      "will\n",
      "0.269724106037\n",
      "price\n",
      "0.267297714618\n",
      "pleas\n",
      "0.2611688867\n",
      "most\n",
      "0.257298197952\n",
      "nbsp\n",
      "0.25394145516\n",
      "lo\n",
      "0.253466524314\n",
      "ga\n",
      "0.248296990456\n",
      "hour\n",
      "0.246404357832\n"
     ]
    }
   ],
   "source": [
    "# Sort the weights and obtin the vocabulary list\n",
    "#weight, idx = sort(model.coef_, 'descend');\n",
    "idx = np.argsort(-model.coef_,) # descend sorting\n",
    "#idx由model的权重参数排序位置组成\n",
    "#print(model.coef_)\n",
    "vocabList = getVocabList();\n",
    "\n",
    "print('\\nTop predictors of spam: \\n');\n",
    "for i in range(15): # 列出权重值最高的15个词\n",
    "    #print(' %-15s (%f) \\n'%(vocabList[idx[i]], weight[i]))\n",
    "    print(vocabList[idx[0,i]])\n",
    "    print(model.coef_[0,idx[0,i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "结果与MATLAB程序结果更接近"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下部分为optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 尝试自己的Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "do \n",
      "you \n",
      "want \n",
      "to \n",
      "make \n",
      "dollarnumb \n",
      "or \n",
      "more \n",
      "per \n",
      "week \n",
      "if \n",
      "you \n",
      "are not in vocabList\n",
      "are \n",
      "motiv \n",
      "and \n",
      "qualifi \n",
      "\n",
      "\n",
      "individu \n",
      "will \n",
      "person \n",
      "demonstr not in vocabList\n",
      "demonstr \n",
      "to \n",
      "you \n",
      "system \n",
      "that \n",
      "will \n",
      "make \n",
      "you \n",
      "dollarnumb \n",
      "\n",
      "\n",
      "number \n",
      "per \n",
      "week \n",
      "or \n",
      "more \n",
      "thi \n",
      "is \n",
      "not \n",
      "mlm not in vocabList\n",
      "mlm \n",
      "call \n",
      "our \n",
      "number \n",
      "hour \n",
      "prerecord not in vocabList\n",
      "prerecord \n",
      "number \n",
      "\n",
      "\n",
      "to \n",
      "get \n",
      "the \n",
      "detail \n",
      "numbernumbernumb not in vocabList\n",
      "numbernumbernumb \n",
      "need \n",
      "peopl \n",
      "who \n",
      "want \n",
      "to \n",
      "make \n",
      "seriou \n",
      "money not in vocabList\n",
      "money \n",
      "\n",
      "\n",
      "make \n",
      "the \n",
      "call \n",
      "and \n",
      "get \n",
      "the \n",
      "fact \n",
      "invest \n",
      "number \n",
      "minut \n",
      "in \n",
      "yourself \n",
      "now \n",
      "numbernumbernumb not in vocabList\n",
      "\n",
      "\n",
      "numbernumbernumb \n",
      "look \n",
      "forward \n",
      "to \n",
      "your \n",
      "call \n",
      "and \n",
      "will \n",
      "introduc \n",
      "you \n",
      "to \n",
      "peopl \n",
      "\n",
      "\n",
      "like \n",
      "yourself \n",
      "who \n",
      "are not in vocabList\n",
      "are \n",
      "current \n",
      "make \n",
      "dollarnumb \n",
      "number \n",
      "plu \n",
      "per \n",
      "week \n",
      "numbernumbernumb not in vocabList\n",
      "\n",
      "\n",
      "numbernumbernumb \n",
      "numberljgvnumbernumberleannumberlrmsnumbernumberwxhonumberqiytnumbernumberrjuvnumberhqcfnumbernumbereidbnumberdmtvlnumb not in vocabList\n",
      "\n",
      "\n",
      "numberljgvnumbernumberleannumberlrmsnumbernumberwxhonumberqiytnumbernumberrjuvnumberhqcfnumbernumbereidbnumberdmtvlnumb \n",
      "\n",
      "\n",
      "=========================\n",
      "\n",
      "[470, 1892, 1808, 1698, 996, 478, 1181, 1063, 1230, 1826, 809, 1892, 1069, 73, 1345, 836, 1851, 1241, 1698, 1892, 1630, 1664, 1851, 996, 1892, 478, 1119, 1230, 1826, 1181, 1063, 1675, 876, 1112, 233, 1190, 1119, 791, 1119, 1698, 707, 1665, 439, 1092, 1229, 1843, 1808, 1698, 996, 1489, 996, 1665, 233, 73, 707, 1665, 607, 868, 1119, 1047, 824, 1895, 1116, 975, 675, 1698, 1894, 233, 73, 1851, 866, 1892, 1698, 1229, 955, 1895, 1843, 386, 996, 478, 1119, 1264, 1230, 1826]\n",
      "\n",
      "Processed spamSample1.txt\n",
      "\n",
      "Spam Classification: 1\n",
      "\n",
      "(1 indicates spam, 0 indicates not spam)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================== Part 6: Try Your Own Emails =====================\n",
    "#  Now that you've trained the spam classifier, you can use it on your own\n",
    "#  emails! In the starter code, we have included spamSample1.txt,\n",
    "#  spamSample2.txt, emailSample1.txt and emailSample2.txt as examples. \n",
    "#  The following code reads in one of these emails and then uses your \n",
    "#  learned SVM classifier to determine whether the email is Spam or \n",
    "#  Not Spam\n",
    "\n",
    "# Set the file to be read in (change this to spamSample2.txt,\n",
    "# emailSample1.txt or emailSample2.txt to see different predictions on\n",
    "# different emails types). Try your own emails as well!\n",
    "filename = 'spamSample1.txt';\n",
    "\n",
    "# Read and predict\n",
    "file_contents = readFile(filename);\n",
    "word_indices  = processEmail(file_contents);\n",
    "#print(word_indices)\n",
    "x             = emailFeatures(word_indices);\n",
    "#print(x)\n",
    "p = model.predict(x)\n",
    "\n",
    "print('\\nProcessed %s\\n\\nSpam Classification: %d\\n'%(filename, p))\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1899)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1899,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
